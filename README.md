# LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync

<p align="center" style="font-size: larger;">
  <a href="https://arxiv.org/abs/2412.04431">Paper</a>
</p>

## ðŸ“– Introduction

<p align="center">
<img src="assets/show_images.jpg" width=95%>
<p>

We present LatentSync, an end-to-end lip sync framework based on audio conditioned latent diffusion models without any intermediate motion representation, diverging from previous diffusion-based lip sync methods based on pixel space diffusion or two-stage generation. Our framework can leverage the powerful capabilities of Stable Diffusion to directly model complex audio-visual correlations.

## Demo

<table class="center">
  <tr style="font-weight: bolder;text-align:center;">
        <td width="50%"><b>Original video</b></td>
        <td width="50%"><b>Lip-synced video</b></td>
  </tr>
  <tr>
    <td>
      <video src=https://github.com/user-attachments/assets/5b4805e6-57e4-408a-b325-c18083bc7dcb controls preload></video>
    </td>
    <td>
      <video src=https://github.com/user-attachments/assets/5b4805e6-57e4-408a-b325-c18083bc7dcb controls preload></video>
    </td>
  </tr>
</table>

## ðŸ“‘ Open-Source Plan

- [ ] Inference code and checkpoints
- [ ] Training code
